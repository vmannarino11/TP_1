{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdb52fd",
   "metadata": {},
   "source": [
    "# Trabajo práctico 3 \n",
    "\n",
    "## Francisco Apezteguía, Valentín Mannarino, Juan Sebastián Navajas Jáuregui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d50e6c",
   "metadata": {},
   "source": [
    "# Parte I Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d778493",
   "metadata": {},
   "source": [
    "### Ejercicio 1 \n",
    "\n",
    "Exploramos la base y anunciamos las variables que creemos relevantes para predecir la pobreza. Para ello, utilizamos el nomenclador https://www.indec.gob.ar/ftp/cuadros/menusuperior/eph/EPH_registro_1T2023.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884c25a",
   "metadata": {},
   "source": [
    "Resumimos las posibles variables de interés para nuestro análisis pertenecientes a la EPH de la parte de hogares:\n",
    "\n",
    "**Identificación**\n",
    "\n",
    "**CODUSU** C (29) Código para distinguir viviendas, permite aparearlas con Hogares y Personas. \n",
    "Permite hacer el seguimiento a través de los trimestres.\n",
    "\n",
    "**NRO_HOGAR** N (1) Código para distinguir Hogares. \n",
    "Permite aparearlos con Personas.\n",
    "\n",
    "**REGION** N (2) Código de región 01 = Gran Buenos Aires 40 = Noroeste\n",
    "41 = Noreste 42 = Cuyo 43 = Pampeana 44 = Patagonia.\n",
    "Será de utilidad para trabajar en la región de interés. \n",
    "\n",
    "**Carcaterísticas de la vivienda**\n",
    "\n",
    "**IV2** N (2) ¿Cuántos ambientes/habitaciones tiene la vivienda en total? (sin contar baño/s, cocina, pasillo/s, lavadero, garage). \n",
    "Tenemos el prior que un hogar más grande puede ser reflejo de una situación de no pobreza.\n",
    " \n",
    "**IV3** N (1) Estado del piso:  1. mosaico / baldosa / madera /\n",
    "cerámica / alfombra 2. cemento / ladrillo fijo 3. ladrillo suelto / tierra. \n",
    "En primera instancia creemos que las viviendas con piso de tierra/ ladrillo suelto, poseen una porbabilidad mayor a ser pobre. \n",
    "\n",
    "**V4** N (2) La cubierta exterior del techo es de... 1. membrana / cubierta asfáltica 2. baldosa / losa sin cubierta 3. pizarra / teja 4. chapa de metal sin cubierta 5. chapa de fibrocemento / plástico 6. chapa de cartón 7. saña / tabla / paja con barro / paja sola 9. N/S. Departamento en propiedad horizontal.\n",
    "Una cubierta exterior del techo cercana a una calificación mayor (exceptuando el 9) creemos que puede ser un indicio de un hogar pobre. \n",
    " \n",
    "**IV6** N (1) Tiene agua.. 1. por cañería dentro de la vivienda 2. fuera de la vivienda pero dentro del terreno 3. fuera del terreno.\n",
    "La manera en que acceden a uno de los recursos más importantes, puede ser un buen indicio de la situación del hogar. \n",
    "\n",
    "**IV8** N (1) ¿Tiene baño/letrina? 1 = Sí 2 = No\n",
    "No poseer baño puede ser un buen indicador de la pobreza del hogar, ambiente indispensable de la casa. \n",
    "\n",
    "**IV10** N (1) El baño tiene... 1. inodoro con botón / mochila /\n",
    "cadena y arrastre de agua 2. inodoro sin botón / cadena y con\n",
    "arrastre de agua (a balde) 3. letrina (sin arrastre de agua)\n",
    "En caso de poseer baño, es importante saber las condiciones en que se encuentra. \n",
    "\n",
    "**IV12_1** N (1) La vivienda está ubicada cerca de basural/es (3 cuadras o menos) 1 = Sí  2 = No\n",
    "Tenemos el prior que vivir cerca de basurales está asociado a valores de hogares bajos por las posibles enfermedades de transmisión. \n",
    "\n",
    "**IV_12_2** N (1) La vivienda está ubicada en zona inundable (en los últimos 12 meses) 1 = Sí 2 = No\n",
    "Tenemos el prior que vivir cerca de zonas inundables está asociado a valores de hogares bajos, por las destrucciones (constantes) que generan las inundaciones.\n",
    "\n",
    "**IV12_3** N (1) La vivienda está ubicada en villa de emergencia (por observación) 1 = Sí 2 = No\n",
    "Creemos que vivir en una villa de emergencia aumenta la probabilidad de que el hogar se encuentre en situación de pobreza.\n",
    "\n",
    "\n",
    "**Características habitacionales del hogar**\n",
    "\n",
    "**II1** N (2) ¿Cuántos ambientes / habitaciones tiene este hogar para su uso exclusivo?\n",
    "Creemos que la cantidad de ambientes puede dar indicios de la situación de pobreza del hogar, un hogar con un ambiente y muchas personas, tiene mayor probabilidad de ser pobre. \n",
    "\n",
    "**II7** N (2) Régimen de tenencia 01 = Propietario de la vivienda y el  terreno 02 = Propietario de la vivienda solamente 03 = Inquilino / arrendatario de la vivienda 04 = Ocupante por pago de impuestos / expensas 05 = Ocupante en relación de dependencia 06 = Ocupante gratuito (con permiso) 07 = Ocupante de hecho (sin permiso) 08 = Está en sucesión.\n",
    "Es una característica que puede ayudar a predecir la situación del hogar, las personas que son propietarias pueden tener menores probabilidades de que su hogar esté en situación de pobreza. \n",
    " \n",
    "**II8** N (1) Combustible utilizado para cocinar 01 = Gas de red 02 = Gas de tubo / garrafa 03 = Kerosene / leña / carbón.\n",
    "Poseer gas de red es un indicio de la zona en que habitas y puede ser estar correlacionado con la situación del hogar.\n",
    "\n",
    "**II9** N (1) Baño (tenencia y uso) 01 = Uso exclusivo del hogar 02 = Compartido con otro/s hogar/es de la misma vivienda 03 = Compartido con otra/s vivienda/s 04 = No tiene baño.\n",
    "Tenemos el prior que tener baño compartido con otros hogares aumenta la probabilidad de que el hogar sea pobre. \n",
    "\n",
    "**Estrategias del hogar**\n",
    "Dentro de estrategias del hogar seleccionamos las variables que creíamos que tienen un mayor poder predictivo para lograr identificar un hogar pobre. Creemos que si en los últimos 3 mees el hogar vivió a base del cobrar de un seguro de desempleo, subsidio, donaciones, préstamos o ventas de pertenencias, tiene una mayor probabilidad de encontrarse en situación de pobreza.\n",
    "¿ En los últimos tres meses, las personas de este hogar han vivido...\n",
    "\n",
    "**V1** N (1) ...de lo que ganan en el trabajo? 1 = Sí 2 = No\n",
    "\n",
    "**V2** N (1) ...de alguna jubilación o pensión? 1 = Sí 2 = No\n",
    "\n",
    "**V4** N (1) ...de seguro de desempleo? 1 = Sí 2 = No\n",
    "\n",
    "**V5** N (1) ...de subsidio o ayuda social (en dinero) del gobierno, iglesias, etc.? 1 = Sí 2 = No\n",
    "\n",
    "**V6** N (1) ...con mercaderías, ropa, alimentos gobierno, iglesias, escuelas, etc.? 1 = Sí 2 = No\n",
    "\n",
    "**V12** N (1) ...cuotas de alimentos o ayuda en dinero \n",
    "de personas que no viven en el hogar? 1 = Sí  2 = No\n",
    "\n",
    "**V14** N (1) ...pedir préstamos a familiares / amigos 1 = Sí 2 = No\n",
    "\n",
    "**V17** N (1) ¿Han tenido que vender alguna de sus pertenencias? 1 = Sí  2 = No\n",
    "\n",
    "**Resumen del hogar**\n",
    "\n",
    "**IX_TOT** N (2) Cantidad de miembros del hogar\n",
    "La relevancia de la variable viene de que los hogares en pobres, en general, son aquellos que tienen más miembros en el hogar. \n",
    "\n",
    "**Ingreso total familiar**\n",
    "\n",
    "**ITF** N (12) Monto de ingreso total familiar\n",
    "El monto de ingreso familiar puede ser un indicador rápido de saber la situación del hogar si lo comparamos con la variable (a crear) de ingreso necesario.\n",
    "\n",
    "**Ingreso per cápita familiar**\n",
    "\n",
    "**IPCF** N (12) Monto de ingreso per cápita familiar\n",
    "El ingreso per cápita familiar es otro índice rápido de poder detectar hogares en situaciones de pobreza.\n",
    "\n",
    "Estas variables se suman aquellas que utilizamos en la base individual: \n",
    "\n",
    "**CH04** N (1) Sexo 1 = Varón, 2 = Mujer\n",
    "\n",
    "**CH06** N (2) ¿Cuántos años cumplidos tiene?\n",
    "\n",
    "**CH07** N (1) ¿Actualmente está... 1 = ... unido? 2 = ... casado? 3 = ... separado/a o divorciado/a? 4 = ... viudo/a? 5 = ... soltero/a?\n",
    "\n",
    "**CH08** N (3) ¿Tiene algún tipo de cobertura médica por la que paga o le descuentan? 1 = Obra social (incluye PAMI) 2 = Mutual / prepaga / servicio de emergencia 3 = Planes y seguros públicos 4 = No paga ni le descuentan 9 = Ns/Nr 12 = Obra social y mutual / prepaga / servicio de emergencia 13 = Obra social y planes y seguros públicos 23 = Mutual / prepaga / servicio de emergencia / Planes y seguros  públicos 123 = Obra social, mutual / prepaga / servicio de emergencia y planes y  seguros públicos\n",
    "\n",
    "**CH09** N (1) ¿Sabe leer y escribir? 1 = Sí 2 = No 3 = Menor de 2 años\n",
    "\n",
    "**NIVEL_ED** N (1) Nivel educativo 1 = Primario incompleto (incluye educación especial) 2 = Primario completo\n",
    "3 = Secundario incompleto 4 = Secundario completo 5 = Superior universitario incompleto 6 = Superior universitario completo\n",
    "7 = Sin instrucción 9 = Ns/Nr\n",
    "\n",
    "**ESTADO** N (1) Condición de actividad 0 = Entrevista individual no realizada  (no respuesta al cuestionario \n",
    " individual) 1 = Ocupado 2 = Desocupado 3 = Inactivo 4 = Menor de 10 años\n",
    "\n",
    "**CAT_INAC** N (1) Categoría de inactividad 1 = Jubilado / Pensionado 2 = Rentista 3 = Estudiante 4 = Ama de casa 5 = Menor de 6 años 6 = Discapacitado 7 = Otros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951af988",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Descargamos la base de la EPH correspondiente al primer trimestre de 2023, a nivel hogares. Trabajamos solo con la región Gran Buenos Aires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af86ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los paquetes que utilizaremos y seteamos el directorio con el que vamos a trabajar\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\Usuario\\OneDrive - Económicas - UBA\\Valentin\\Maestria\\Optativas\\Tercer trimestre\\BIg Data\\Trabajo Práctico\\Tp3\")\n",
    "\n",
    "# Importamos la base de datos a nivel hogar\n",
    "data_eph_h = pd.read_excel(\"usu_hogar_T123.xlsx \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8ba240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajamos con las observaciones que son de la región Gran Buenos Aires\n",
    "data_eph_h=data_eph_h[data_eph_h[\"REGION\"]==1]\n",
    "data_eph_h_prueba = data_eph_h.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c014a6c",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Importamos la base de datos individual para unirla con la de hogares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a235dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la base de datos a nivel individual\n",
    "data_eph_i = pd.read_excel(\"usu_individual_T123.xlsx \")\n",
    "\n",
    "# Trabajamos con las observaciones que son de la región Gran Buenos Aires\n",
    "data_eph_i=data_eph_i[data_eph_i[\"REGION\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1793652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En la base Hogar (archivo usu_hogar.txt) todos los hogares que pertenecen a una \n",
    "# misma vivienda poseen el mismo CODUSU. Para identificar los hogares se debe utilizar \n",
    "# CODUSU y NRO_HOGAR.\n",
    "\n",
    "# En la de base de individuos (archivo usu_individual.txt) todos los miembros del hogar tienen el \n",
    "# mismo CODUSU y NRO_HOGAR.\n",
    "\n",
    "# De esta forma, entendemos que debemos hacer el unión en base a CODUSU.\n",
    "\n",
    "# Realiza el merge, para aquellas variables repetidas se agregará un sufijo y mantendrá ambas variables\n",
    "data_eph = data_eph_h.merge(data_eph_i, on='CODUSU', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca94d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos todas las columnas que terminen con _y que son un duplicado\n",
    "data_eph = data_eph.filter(regex='^(?!.*_y$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7ade21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombra las columnas que terminan con \"_x\" eliminando la extensión\n",
    "data_eph = data_eph.rename(columns=lambda x: x.rstrip('_x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8798cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análizamos en un excel y generamos una copia del archivo para ver el resultado\n",
    "#nombre_archivo = \"data_eph.xlsx\"\n",
    "#data_eph.to_excel(nombre_archivo, index=False)\n",
    "#data_eph_copia = data_eph.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f813cc",
   "metadata": {},
   "source": [
    "### Ejercicio 4 \n",
    "\n",
    "A continuación generamos funciones para realizar la limpieza de la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc79345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_valores(base, variables):\n",
    "    '''\n",
    "    La función elimina aquellas observaciones que tengan un valor menor a cero en las variables especificadas.\n",
    "    Input:\n",
    "        base: DataFrame con los datos.\n",
    "        variables: Lista de nombres de las variables a filtrar.\n",
    "    Output:\n",
    "        DataFrame con las observaciones filtradas.\n",
    "    '''\n",
    "    # Creamos una copia del DataFrame original para hacer la limpieza\n",
    "    df_filtrado = base.copy()\n",
    "\n",
    "    # Iteramos a través de las variables y filtramoslas observaciones\n",
    "    for variable in variables:\n",
    "        df_filtrado = df_filtrado[df_filtrado[variable] >= 0]\n",
    "\n",
    "    return df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8541dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers (base, variables):\n",
    "    '''\n",
    "    La función elimina aquellas observaciones que tengan valores de ingresos superiores en 50 veces la media de la población.\n",
    "    Input:\n",
    "        base: DataFrame con los datos.\n",
    "        variables: Lista de nombres de las variables a filtrar.\n",
    "    Output:\n",
    "        DataFrame con las observaciones filtradas.\n",
    "    '''\n",
    "    # Creamos una copia del DataFrame original para hacer la limpieza\n",
    "    df_filtrado = base.copy()\n",
    "    # Iteramos a través de las variables y filtramos las observaciones\n",
    "    for variable in variables:\n",
    "        media = df_filtrado[variable].mean()  \n",
    "        limite_superior = media * 50  # Establecemos el límite superior\n",
    "        df_filtrado = df_filtrado[df_filtrado[variable] <= limite_superior]\n",
    "\n",
    "    return df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8cd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_missings(base, umbral):\n",
    "    '''\n",
    "    La función elimina las columnas que tienen más valores faltantes respecto al \"umbral\".\n",
    "    Input:\n",
    "        base: DataFrame con los datos.\n",
    "        umbral: Umbral de valores faltantes permitidos.\n",
    "    Output:\n",
    "        DataFrame con las columnas eliminadas.\n",
    "    '''\n",
    "    # Creamos una copia del DataFrame original para hacer la limpieza\n",
    "    df_filtrado = base.copy()\n",
    "    # Pedimos que elimine las columnas que superen el umbral\n",
    "    columnas_a_eliminar = [columna for columna in base.columns if base[columna].isna().sum() > umbral]\n",
    "    df_filtrado = df_filtrado.drop(columnas_a_eliminar, axis=1)\n",
    "   \n",
    "    return df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17a2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_valores_9(base, variables):\n",
    "    '''\n",
    "    La función elimina aquellas observaciones que tengan un valor mayor o igual a nueve en las variables especificadas.\n",
    "    Input:\n",
    "        base: DataFrame con los datos.\n",
    "        variables: Lista de nombres de las variables a filtrar.\n",
    "    Output:\n",
    "        DataFrame con las observaciones filtradas.\n",
    "    '''\n",
    "    # Creamos una copia del DataFrame original para hacer la limpieza\n",
    "    df_filtrado = base.copy()\n",
    "\n",
    "    # Iteramos a través de las variables y filtramos las observaciones\n",
    "    for variable in variables:\n",
    "        df_filtrado = df_filtrado[df_filtrado[variable] <= 9]\n",
    "\n",
    "    return df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc6674",
   "metadata": {},
   "source": [
    "### Ejercicio 5 \n",
    "Limpiamos la base de datos con las funciones creadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadfe35",
   "metadata": {},
   "source": [
    "Utilizamos nuestra función del punto previo para eliminar edades o ingresos menores a cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7387d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_filtro = ['ITF','IPCF', 'P21', 'P47T', 'CH06']\n",
    "data_eph = eliminar_valores (data_eph, variables_filtro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953f0d1",
   "metadata": {},
   "source": [
    "Utilizamos la función creada previamente para eliminar los outliers de las variables de ingreso de nuestra base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43ea4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_filtro = ['ITF','IPCF', 'P21', 'P47T']\n",
    "data_eph = outliers (data_eph, variables_filtro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff0bf2",
   "metadata": {},
   "source": [
    "Decidimos quedarnos con aquellas variables que tengan hasta un máximo de 350 valores faltantes.\n",
    "Tomamos esta decisión teniendo en mente el trade off entre la complejidad que puede tener el modelo (cantidad de variables explicativas)\n",
    "y la problemática que existe al no poder calcular modelos de predicción con valores missing. \n",
    "En caso de incluir estas variables, deberíamos sacrificar una gran cantidad de observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bd8683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "umbral = 350 \n",
    "data_eph = eliminar_missings(data_eph, umbral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98eaab",
   "metadata": {},
   "source": [
    "Utilizamos la cuarta función para aquellas variables que tengan un valor mayor a 9 ya que en general este valor indica que las personas no respondieron a la pregunta de la encuesta\n",
    "Departamento en propiedad horizontal no la eliminamos porque implicaría quitar cerca del 10% de las observaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0029d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_filtro = ['II7','V1','V2','V4','V5','V6','V12','V14','V17','CH07','NIVEL_ED']\n",
    "data_eph = eliminar_valores_9 (data_eph, variables_filtro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1f6b7",
   "metadata": {},
   "source": [
    "Hacemos limpieza de algunas variables que no cumplen con los criterios establecidos por nuestras funciones. \n",
    "Por ejemplo, CH08 tiene el 9 como no respuesta pero luego tiene valores mayores que tienen asociados respuestas con utilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f93749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eph = data_eph[data_eph['CH08'] != 9]\n",
    "data_eph = data_eph[data_eph['IV2'] != 99]\n",
    "data_eph = data_eph[data_eph['II1'] != 99]\n",
    "data_eph = data_eph[data_eph['II9'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc5165d",
   "metadata": {},
   "source": [
    "A continuación, filtramos nuestra base de datos para seleccionar las variables que serán utilizadas en los siguientes ejercicios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a391664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la lista de variables de interés.\n",
    "variables_de_interes = [\n",
    "    'CODUSU',\n",
    "    'PONDIH',\n",
    "    'IV2',\n",
    "    'IV3',\n",
    "    'V4',\n",
    "    'IV6',\n",
    "    'IV8',\n",
    "    'IV10',\n",
    "    'IV12_1',\n",
    "    'IV12_2',\n",
    "    'IV12_3',\n",
    "    'II1',\n",
    "    'II7',\n",
    "    'II8',\n",
    "    'II9',\n",
    "    'V1',\n",
    "    'V2',\n",
    "    'V4',\n",
    "    'V5',\n",
    "    'V6',\n",
    "    'V12',\n",
    "    'V14',\n",
    "    'V17',\n",
    "    'ITF',\n",
    "    'IX_TOT',\n",
    "    'IPCF',\n",
    "    'P47T',\n",
    "    'CH04',\n",
    "    'CH06',\n",
    "    'CH07',\n",
    "    'CH08',\n",
    "    'CH09',\n",
    "    'NIVEL_ED',\n",
    "    'ESTADO',\n",
    "    'CAT_INAC'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "248765da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las variables de interés\n",
    "data_eph = data_eph[variables_de_interes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c49c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODUSU       object\n",
      "PONDIH        int64\n",
      "IV2           int64\n",
      "IV3           int64\n",
      "V4            int64\n",
      "V4            int64\n",
      "IV6           int64\n",
      "IV8           int64\n",
      "IV10          int64\n",
      "IV12_1        int64\n",
      "IV12_2        int64\n",
      "IV12_3        int64\n",
      "II1           int64\n",
      "II7           int64\n",
      "II8           int64\n",
      "II9           int64\n",
      "V1            int64\n",
      "V2            int64\n",
      "V4            int64\n",
      "V4            int64\n",
      "V5            int64\n",
      "V6            int64\n",
      "V12           int64\n",
      "V14           int64\n",
      "V17           int64\n",
      "ITF           int64\n",
      "IX_TOT        int64\n",
      "IPCF        float64\n",
      "P47T        float64\n",
      "CH04          int64\n",
      "CH06          int64\n",
      "CH07          int64\n",
      "CH08        float64\n",
      "CH09          int64\n",
      "NIVEL_ED      int64\n",
      "ESTADO        int64\n",
      "CAT_INAC      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos los tipos de variables en la lista\n",
    "tipos_variables = data_eph[variables_de_interes].dtypes\n",
    "\n",
    "# Mostrar los tipos de variables\n",
    "print(tipos_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1d6df",
   "metadata": {},
   "source": [
    "No observamos variables categóricas o strings que debamos modificar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb987a",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "Presentamos estadísticas descriptivas de cinco variables de la encuesta hogar que creemos que pueden ser relevantes para predecir la pobreza.\n",
    "\n",
    "**IV3** Piso interior de la casa. Tener un piso de tierra puede ser muy influyente en la situación del hogar. Cuanto mayor sea el valor de esta variable, menor será la calidad del piso del interior de la casa. \n",
    "\n",
    "**IV6** Forma de conseguir agua. La manera de acceder a un recurso tan importante puede indicarte la situación del hogar. Cuanto mayor sea la variable, mayor es el esfuerzo diario que la persona debe hacer para tener acceso al agua. \n",
    "\n",
    "**IV8** Tiene baño. La no posibilidad de tener un baño, puede ser un buen indicador para analizar la situación del hogar. Es un una dummie que toma valor 1 si la persona tiene baño y 2 si no tiene acceso. \n",
    "\n",
    "**IX_TOT** Cantidad de habitantes. Tal como explicamos anteriormente, sabemos que los hogares pobres suelen tener una mayor cantidad de habitantes.\n",
    "\n",
    "**IPCF** Monto de ingreso per cápita familiar. El ingreso per cápita familiar es un buen reflejo de la situaciones de los indiviudos dentro del hogar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2190a0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IV3</th>\n",
       "      <td>5987.0</td>\n",
       "      <td>1.134625</td>\n",
       "      <td>0.359932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IV6</th>\n",
       "      <td>5987.0</td>\n",
       "      <td>1.029898</td>\n",
       "      <td>0.179862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IV8</th>\n",
       "      <td>5987.0</td>\n",
       "      <td>1.002171</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IX_TOT</th>\n",
       "      <td>5987.0</td>\n",
       "      <td>3.778854</td>\n",
       "      <td>1.892147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPCF</th>\n",
       "      <td>5987.0</td>\n",
       "      <td>58359.319058</td>\n",
       "      <td>78014.010937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37333.33</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>980000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count          mean           std  min  25%       50%      75%  \\\n",
       "IV3     5987.0      1.134625      0.359932  1.0  1.0      1.00      1.0   \n",
       "IV6     5987.0      1.029898      0.179862  1.0  1.0      1.00      1.0   \n",
       "IV8     5987.0      1.002171      0.046551  1.0  1.0      1.00      1.0   \n",
       "IX_TOT  5987.0      3.778854      1.892147  1.0  2.0      4.00      5.0   \n",
       "IPCF    5987.0  58359.319058  78014.010937  0.0  0.0  37333.33  79000.0   \n",
       "\n",
       "             max  \n",
       "IV3          4.0  \n",
       "IV6          3.0  \n",
       "IV8          2.0  \n",
       "IX_TOT      13.0  \n",
       "IPCF    980000.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "variables = data_eph[['IV3', 'IV6', 'IV8', 'IX_TOT', 'IPCF']]\n",
    "variables.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46460c33",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "Agregamos a nuestra base los valores de adultos equivalente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91870ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: modificamos la tabla para que a cada edad desde 0 hasta 99 años se corresponda, según su sexo, con la proporción del adulto equivalente\n",
    "# Importamos la base de datos con la que trabajaremos la cual se adjunta en GitHub. \n",
    "data_equiv = pd.read_excel(\"tabla_adulto_equiv.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf1124fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función lógica para que dentro de data_eph en caso que la persona sea un hombre (CH04=1) se asigne el valor \n",
    "# de la variable Varones dentro de la base de datos de adulto equivalente, haciendo coincidir la edad en data_eph con la de\n",
    "# la variable Edad dentro de la base de datos de adulto equivalente. Realizamos el mismo procedimiento para las mujeres\n",
    "data_eph['adulto_equiv'] = None\n",
    "def assign_data_equiv(row):\n",
    "    if row['CH04'] == 1:\n",
    "        return data_equiv.loc[data_equiv['Edad'] == row['CH06'], 'Varones'].values[0]\n",
    "    elif row['CH04'] == 2:\n",
    "        return data_equiv.loc[data_equiv['Edad'] == row['CH06'], 'Mujeres'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Luego, realizamos este procedimiento para todas las filas de nuestra base de datos\n",
    "data_eph['adulto_equiv'] = data_eph.apply(assign_data_equiv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c9210da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de valores faltantes en la columna 'adulto_equiv': 0\n",
      "count    5987.000000\n",
      "mean        0.799758\n",
      "std         0.152924\n",
      "min         0.370000\n",
      "25%         0.690000\n",
      "50%         0.770000\n",
      "75%         1.000000\n",
      "max         1.040000\n",
      "Name: adulto_equiv, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos un resumen de nuestra variable creada para chequear que no tengamos valores raros o missing values\n",
    "resumen_adulto = data_eph['adulto_equiv'].describe() \n",
    "missing_values_in_adulto_equiv = data_eph['adulto_equiv'].isnull().sum()\n",
    "\n",
    "print(\"Cantidad de valores faltantes en la columna 'adulto_equiv':\", missing_values_in_adulto_equiv)\n",
    "print(resumen_adulto)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f5bd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último, creamos la nueva variable adulto_equiv_hog que le asigna la suma de adultos equivalentes en el hogar a cada observación\n",
    "data_eph['ad_equiv_hogar'] = data_eph.groupby('CODUSU')['adulto_equiv'].transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9c992",
   "metadata": {},
   "source": [
    "### Ejercicio 8\n",
    "Reporte del ingreso total familiar\n",
    "\n",
    "Nota: en nuestro código, ya hemos eliminado algunas observaciones que presentaban un ns/nr en la variable indicativa del ingreso personal (P47T). En general, estas observaciones también tenían también un valor igual a 0 en la variable ingreso total familiar. \n",
    "\n",
    "Buscamos el ingreso necesario a nivel familiar para no ser pobre para aquellos grupos de personas que respondieron el ingreso total familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48e1bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de personas que no respondieron cuál es su ingreso total familiar es igual a 1793\n"
     ]
    }
   ],
   "source": [
    "# Contamos la cantidad de veces que la variable Ingreso Total Familiar (ITF) es igual a 0\n",
    "count_IFT0 = (data_eph['ITF'] == 0).sum()\n",
    "\n",
    "print(f\"La cantidad de personas que no respondieron cuál es su ingreso total familiar es igual a {count_IFT0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a5ab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame llamado \"respondieron\" con las observaciones donde la variable Ingreso total familiar es mayor que 0\n",
    "respondieron = data_eph[data_eph['ITF'] > 0]\n",
    "\n",
    "# Creamos un DataFrame llamado \"no_respondieron\" con las observaciones donde la variable Ingreso total familiar  es igual a 0\n",
    "no_respondieron = data_eph[data_eph['ITF'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45a5238d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5288\\2610509039.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * 57371.05\n"
     ]
    }
   ],
   "source": [
    "# Agregamos la variable \"ingreso_necesario\" al DataFrame \"respondieron\" según el criterio específicado en la consigna\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * 57371.05\n",
    "#respondieron.loc[:, 'ingreso_necesario'] = respondieron['ad_equiv_hogar'] * 57371.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94a316",
   "metadata": {},
   "source": [
    "### Ejercicio 9\n",
    "Agregamos a la base respondieron una columna llamada pobre, que toma valor 1 si el ITF es menor al ingreso necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839aff94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5288\\2159865694.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  respondieron.loc[:,'pobre'] = (respondieron['ITF'] <= respondieron['ingreso_necesario']).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Ahora agregamos la variable pobre, según si cumple la condición de que el ingreso familiar total para cada individuo es menor \n",
    "# al necesario para cubrir la canasta básica total (CBT)\n",
    "respondieron.loc[:,'pobre'] = (respondieron['ITF'] <= respondieron['ingreso_necesario']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbd771b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de pobres en la muestra es de 1609\n",
      "El porcentaje de pobres de la muestra es de 38.36%\n"
     ]
    }
   ],
   "source": [
    "# Finalmente, contamos la cantidad de individuos que pueden ser considerados pobres\n",
    "cantidad_de_pobres = respondieron['pobre'].sum()\n",
    "total_observaciones = len(respondieron)\n",
    "porcentaje_pobres = (cantidad_de_pobres / total_observaciones) * 100\n",
    "print(f\"La cantidad de pobres en la muestra es de {cantidad_de_pobres}\")\n",
    "print(f\"El porcentaje de pobres de la muestra es de {porcentaje_pobres:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adc115",
   "metadata": {},
   "source": [
    "### Ejercicio 10\n",
    "En este ejercicio utilizaremos el ponderador PONDIH para estimar la cantidad de hogares pobres en el Gran Buenos Aires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "929448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos los datos por el código del hogar (CODUSU)\n",
    "data_eph_agrupada = respondieron.groupby('CODUSU').first().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b591e",
   "metadata": {},
   "source": [
    "Creamos la variable \"ponde_pobre\" que se forma como la multiplicación del estado de ese hogar por la cantidad que pondera (PONDIH), de manera tal, que si sumamos este ponderador nos daría la cantidad de pobres. Para obtener la proporción, lo dividimos por la sumatoria de PONDIH. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64386dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de pobres en la muestra es de 1.524.503\n",
      "El porcentaje de pobres de la muestra es de 28.87%\n"
     ]
    }
   ],
   "source": [
    "data_eph_agrupada['ponde_pobre'] = data_eph_agrupada['PONDIH'] * data_eph_agrupada['pobre']\n",
    "cantidad_de_pobres = data_eph_agrupada['ponde_pobre'].sum()\n",
    "total_poblacion = data_eph_agrupada['PONDIH'].sum()\n",
    "porcentaje_pobres = (cantidad_de_pobres / total_poblacion) * 100\n",
    "cantidad_de_pobres_formateada = '{:,}'.format(cantidad_de_pobres).replace(',', '.')\n",
    "print(f\"La cantidad de pobres en la muestra es de {cantidad_de_pobres_formateada}\")\n",
    "print(f\"El porcentaje de pobres de la muestra es de {porcentaje_pobres:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc01f8",
   "metadata": {},
   "source": [
    "En este caso, el informe del INDEC indica que los hogares en situación de pobreza para Buenos Aires y el Gran Buenos Aires es igual al 30,3. Este valor es similar al calculado por nosotros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92bc96",
   "metadata": {},
   "source": [
    "Guardamos para trabjar después una copia de la base repsondieron y no repsondieron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84d19398",
   "metadata": {},
   "outputs": [],
   "source": [
    "respondieron_copia = respondieron.copy()\n",
    "no_respondieron_copia = no_respondieron.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b1a07",
   "metadata": {},
   "source": [
    "# Parte II Construcción de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d6a67",
   "metadata": {},
   "source": [
    "### Ejercicio 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias necesarias:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm     \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ce643",
   "metadata": {},
   "source": [
    "Creamos una función que tome como input al modelo y a las muestras de entrenamiento y test. Dentro de la función indicamos que se ajuste el modelo a partir de la muestra de entrenamiento y luego genere las predicciones en la muestra de test. A partir de este procedimiento calculamos las métricas. Basicamente, esta función evalua qué tan bueno es el modelo dado para predecir un outcome de interes. Dentro del modelo input de la función, podemos setear el de k vecinos más cercanos, el de regresión logística o el de análisis discriminante junto con sus parámetros de interes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(modelo, x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    La función realiza una evaluación de los modelos, ajustados, para luego expulsar las métricas\n",
    "    de matriz de confusión, curvas ROC, AUC, accuracy score y ecm. \n",
    "    Input:\n",
    "        modelo, x_train, x_test, y_train, y_test\n",
    "    Output:\n",
    "        matriz de confusión, curvas ROC, AUC, accuracy score y ecm\n",
    "    '''\n",
    "    \n",
    "    modelo.fit(x_train, y_train) # Entrenamos el modelo en las muestras de entrenamiento\n",
    "    y_pred = modelo.predict(x_test) # Generamos las predicciones a partir de nuestras variables explicativas\n",
    "    \n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred) # Generamos la matriz de confusión\n",
    "    ecm = mean_squared_error(y_test, y_pred) # Generamos el ECM\n",
    "    auc = roc_auc_score(y_test, y_pred) # Genermaos el valor del AUC\n",
    "    accuracy = accuracy_score(y_test, y_pred) # Generamos el valor de precisión\n",
    "    #df_metrics = pd.DataFrame({'modelo': [modelo], 'accuracy': [accuracy], 'auc':[auc], 'ecm':[ecm]})\n",
    "\n",
    "    #fpr, tpr, thresholds = roc_curve(y_test, y_pred) # Generamos la curva ROC\n",
    "    #display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='')\n",
    "    #display.plot()  \n",
    "    #plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    #plt.show() \n",
    "\n",
    "    return matriz_confusion, auc, accuracy, ecm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d88ca",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Esta función tiene como inputs el modelo, la cantidad de particiones que querramos realizar para cross validation y las muestras de interes (variable explicada y explicativas). En primer lugar le indicamos que realice una partición aleatoria y que para hacerlo no tome las observaciones contiguas. Luego iniciamos un loop para cada forma de particionar la muestra (si tenemos un k=5 este loop itera 5 veces, una vez por distinto tipo de partición) donde indicamos que parte de las particiones se utilizaran como prueba y parte como test. En el siguiente paso utilizamos la función del ejercicio 1, indicando como output las pruebas de test y de entrenamiento que creamos previamente y nos quedamos con el error cuadrático medio calculado a partir de la función. Guardamos en un data frame la información del error cuadrático medio para cada modelo y para cada partición. Finalmente, calculamos el error cuadrático medio del modelo y calibramos para que este sea nuestro outcome, junto con el data frame previamente creado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(modelo, k, x, y):\n",
    "    '''\n",
    "    La función realiza cross-validation con k iteraciones de los modelos.\n",
    "    Input:\n",
    "        modelo, k, x, y\n",
    "    Output:\n",
    "        métricas de evaluación para cada partición.\n",
    "    '''\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=10) # Determinamos la forma en que se particiona la muestra\n",
    "    metrics_list = pd.DataFrame(columns=[\"modelo\", \"particion\", \"ecm\"]) # Creamos nuestra data frame\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)): # Iteramos a través de cada partición de la muestra\n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index] # Indicamos la partición de las variables explicativas que estarán en el entrenamiento y test\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index] # Indicamos la partición de la variable explicada que estarán en el entrenamiento y test\n",
    "        \n",
    "        resultados = evalua_metodo(modelo, x_train, y_train, x_test, y_test) # Utilizamos la función previamente creada para la partición dentro de la iteración.\n",
    "        ecm = resultados[3] # Nos quedamos con el ecm para esa partición\n",
    "        metrics_list = pd.concat([metrics_list, pd.DataFrame({\"modelo\": [str(modelo)], \"particion\": [i], \"ecm\": [ecm]})], ignore_index=True) # Creamos el data frame con nuestra información\n",
    "    \n",
    "    final_ecm = metrics_list[\"ecm\"].mean() # Calculamos el error cuadrático medio para el modelo que analizamos\n",
    "    \n",
    "    return final_ecm, metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb47fb",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Esta función tendrá el objetivo de analizar el parámetro óptimo en la regularización de una regresión logística utilizando Lasso y Ridge. Tiene como inputs el hiperparámetro lambda a analizar, junto con la cantidad de particiones que busquemos hacer para la cross validation y las variables explicativas y explicada. En primer lugar, debido a que ambos métodos de regularización son sensibles a la escala de las variables, estándarizamos nuestras variables explicativas.\n",
    "Luego, iniciamos un loop sobre cada posible valor que tome el lambda y realizamos dos procesos distintos con estos valores: uno para Ridge y otro para Lasso. En ambos utilizamos nuestra función de cross validation tomando como inputs el modelo de regresión logística a partir de las dos posibles configuraciones. Lo que obtenemos es el promedio del error cuadrático medio para cada uno de los valores de lambda. Finalmente, creamos una base de datos con los valores del error cuadrático medio promedio para cada lambda y calculamos el error cuadrático medio mínimo para cada uno de los métodos de regularización. El outcome de nuestra función es el mínimo error cuadrático medio para cada método y entre métodos, la base de datos que creamos y la especificación de cuál método es el que nos resulta en el lambda con menor error cuadrático medio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c640092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def evalua_config(hiperparametro, k, x, y):\n",
    "    '''\n",
    "    Esta función toma cómo input una serie de valores de Lambda y el número de particiones (k) \n",
    "    para Cross Validation y devuelve los valores de lambda que minimizan el ECM usando Lasso y Ridge para reg_logit\n",
    "    Input:\n",
    "        hiperparametro, k, x, y\n",
    "    Output:\n",
    "        mínimo ECM entre Lasso y Ridge, Mínimo ECM de Lasso, Min ECM de Ridge, data frame.\n",
    "    '''\n",
    "    ret_lasso = {}\n",
    "    ret_ridge = {}\n",
    "    resultados = []\n",
    "    resultados_lasso = pd.DataFrame() \n",
    "    resultados_ridge = pd.DataFrame()\n",
    "       \n",
    "    sc = StandardScaler()\n",
    "    # Estandarizamos las observaciones \n",
    "    x_transformed = pd.DataFrame(sc.fit_transform(x),index=x.index, columns=x.columns)\n",
    "    \n",
    "    for elem in hiperparametro: #iteramos a través de los valores de lambda que determinamos\n",
    "        # Lasso\n",
    "        clf_lasso_LR = LogisticRegression(C=1/elem, penalty=\"l1\", solver=\"saga\", tol=0.01) # Llamamos a nuestro modelo lasso y le indicamos que use como \n",
    "                                                                                           # parámetro el valor de lambda de la iteración\n",
    "        ecm_lasso, result_lasso = cross_validation(clf_lasso_LR, k, x_transformed, y) # Usamos nuestra función para hacer cross validation entre los valores de lambda\n",
    "        ret_lasso[elem] = ecm_lasso # Guardamos el error cuadrático medio para cada uno de los lambda\n",
    "        result_lasso['Lambda'] = elem\n",
    "        resultados_lasso = pd.concat([resultados_lasso, result_lasso], ignore_index=True)\n",
    "        \n",
    "        # Ridge (realizamos el mismo procedimiento que con Lasso)\n",
    "        clf_ridge_LR = LogisticRegression(\n",
    "            C=1/elem, penalty=\"l2\", solver=\"saga\", tol=0.01)\n",
    "        ecm_ridge, result_ridge = cross_validation(clf_ridge_LR, k, x_transformed, y)\n",
    "        ret_ridge[elem] = ecm_ridge\n",
    "        result_ridge['Lambda'] = elem\n",
    "        resultados_ridge = pd.concat([resultados_ridge, result_ridge], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        resultados.append([elem, ecm_lasso, ecm_ridge]) # Agrupamos el error cuadrático medio de Ridge y Lasso para cada lambda\n",
    "        resultados_df = pd.DataFrame(resultados, columns=['Lambda', 'ECM_Lasso', 'ECM_Ridge']) # Creamos un data frame con estos valores\n",
    "\n",
    "    min_lasso = min(ret_lasso, key=ret_lasso.get) # Calculamos el mínimo error cuadrático medio en Lasso\n",
    "    min_ridge = min(ret_ridge, key=ret_ridge.get) # Calculamos el mínimo error cuadrático medio en Ridge\n",
    "    \n",
    "    if ret_lasso[min_lasso] < ret_ridge[min_ridge]:\n",
    "        tipo_regularizacion = \"Lasso\"\n",
    "        hiperparametro_optimo = min_lasso\n",
    "    else:\n",
    "        tipo_regularizacion = \"Ridge\"\n",
    "        hiperparametro_optimo = min_ridge\n",
    "        \n",
    "    return min(min_lasso, min_ridge), min_lasso, min_ridge, resultados_df, tipo_regularizacion, resultados_ridge, resultados_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301acc4",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "Esta función tiene como objetivo evaluar de manera simultánea distintos métodos para hacer clasificación. Tiene como inputs la cantidad de particiones para realizar cross validation, la cantidad de vecinos cercanos a utilizar en ese modelo, el valor de lambda para regularizar la regresión logística, las variables explicativas y explicada. En primer lugar definimos los distintos modelos a utilizar y particionamos la muestra entre entrenamiento y test. Además, estándarizamos las variables explicativas de test y entrenamiento ya que serán utilizadas para el modelo de regresión logística. \n",
    "En el segundo paso iteramos dentro de cada modelo. En el de regresión logística en primer lugar buscamos el parámetro óptimo lambda a partir de nuestra función evalua_config, luego entrenamos nuestro modelo con lasso o ridge según cuál método haya arrojado el menor error cuadrático medio y con la función evalua_metodo calculamos las métricas. Realizamos el mismo procedimiento para k vecinos más cercanos, indicando la cantidad de vecinos y también para el análisis de discriminante líneal. El outcome es el error cuadrático medio, la AUC y la presición para cada uno de los modelos con su respectivo parámetro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef evalua_multiples_metodos(k_cv, k_knn, hiperparam, x, y):\n",
    "    '''\n",
    "    Esta función evalúa modelos de Regresión Logística, K Vecinos Cercanos y Análisis Discriminante  \n",
    "    Input:\n",
    "        k_cv, k_knn, hiperparam, x_train, x_test, y_train, y_test, x, y\n",
    "    Output:\n",
    "        matriz con los resultados de los modelos analizados.\n",
    "    '''\n",
    "    modelos = ['regresion_logistica', 'k_vecinos_cercanos', 'analisis_discriminante']\n",
    "    \n",
    "    matriz = pd.DataFrame(columns=[\"Modelo\", \"Hiperparametro\", \"Precisión\", \"AUC\", \"ECM\"])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "    \n",
    "    #Estandarizamos las variables para la regresión logística\n",
    "    sc = StandardScaler() \n",
    "    x_transformed_train = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "    x_transformed_test = pd.DataFrame(sc.fit_transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "    \n",
    "    for modelo in modelos:\n",
    "        if modelo == 'regresion_logistica':\n",
    "            hiperparametro_optimo, _, _, _, tipo_regularizacion,_,_ = evalua_config(hiperparam, k_cv, x, y)\n",
    "\n",
    "            if tipo_regularizacion == \"Lasso\":\n",
    "                log_reg = LogisticRegression(penalty=\"l1\", C=1/hiperparametro_optimo, solver=\"saga\", max_iter=1000)\n",
    "            else:\n",
    "                log_reg = LogisticRegression(penalty=\"l2\", C=1/hiperparametro_optimo, solver=\"saga\", max_iter=1000)\n",
    "            log_reg.fit(x_transformed_train, y_train)\n",
    "            metricas_log = evalua_metodo(log_reg, x_transformed_train, y_train, x_transformed_test, y_test)\n",
    "            results = [modelo, hiperparametro_optimo, metricas_log[2], metricas_log[1], metricas_log[3]]\n",
    "            matriz.loc[len(matriz)] = results\n",
    "\n",
    "        if modelo == 'k_vecinos_cercanos':\n",
    "            k = k_knn\n",
    "            x_test_a = np.array(x_test)\n",
    "            x_train_a = np.array(x_train)\n",
    "            k_vecinos = KNeighborsClassifier(n_neighbors=k)\n",
    "            k_vecinos.fit(x_train_a, y_train)\n",
    "            matriz_confusion, auc, accuracy, ecm = evalua_metodo(k_vecinos, x_train_a, y_train, x_test_a, y_test)\n",
    "            results = [modelo, k_knn, accuracy, auc, ecm]\n",
    "            matriz.loc[len(matriz)] = results\n",
    "\n",
    "        if modelo == 'analisis_discriminante':\n",
    "            an_dis = LinearDiscriminantAnalysis().fit(x_train, y_train)\n",
    "            metricas_ad = evalua_metodo(an_dis, x_train, y_train, x_test, y_test)\n",
    "            results = [modelo, \"NA\", metricas_ad[2], metricas_ad[1], metricas_ad[3]]\n",
    "\n",
    "            matriz.loc[len(matriz)] = results\n",
    "    \n",
    "    return matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76a58a",
   "metadata": {},
   "source": [
    "# Parte III: Clasificación y regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870e44e",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Eliminar de las bases respondieron y no respondieron las variables relacionadas a ingresos, adulto_equiv, ad equiv hogar e ingreso necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d19043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la lista de columnas de la base no_respondieron\n",
    "columnas = no_respondieron.columns.tolist()\n",
    "# Imprimimos la lista de columnas\n",
    "#print(columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el vector de variables a eliminar\n",
    "# Eliminamos el identificador, ponderador, variables de ingreso, adulto_equiv y ad_equiv_hogar.\n",
    "# Las otras variables fueron eliminadas según lo explicado en la Parte I ejercicio 5.\n",
    "columnas_a_eliminar = ['CODUSU', 'ITF','PONDIH', 'IPCF', 'P47T','adulto_equiv','ad_equiv_hogar']\n",
    "respondieron = respondieron.drop (columns=columnas_a_eliminar)\n",
    "respondieron = respondieron.drop(columns=['ingreso_necesario'])\n",
    "no_respondieron = no_respondieron.drop (columns=columnas_a_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos las variables que nos quedaron\n",
    "columnas = no_respondieron.columns.tolist()\n",
    "columnas_2 = respondieron.columns.tolist()\n",
    "# print(columnas, columnas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos a la variable pobre como nuestra variable dependiente\n",
    "y = respondieron['pobre']\n",
    "# Definimos las variables independientes (X) excluyendo 'pobre'\n",
    "x = respondieron.drop(columns=['pobre'])\n",
    "# Agregamos la columna de la constante\n",
    "x = pd.concat([pd.Series(1, index=x.index, name='constante'), x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be338741",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Corremos la función evalua_multiples_metodos con la base respondieron utilizando valores arbitrarios. En este caso, utilizamos la cantidad de 5 vecinos cercanos y 5 particiones de muestra en cross validation. Además, elegimos un único parámetro lambda en logistic regression igual a 0.6 (la función al tener un solo parámetro lo elegirá como el óptimo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos el valor de cv, vecinos más cercanos y lambda \n",
    "k_cv = 5\n",
    "k_knn = 5\n",
    "hiperparametro = [0.6] \n",
    "\n",
    "# Llamamos a la función evalua_multiples_metodos con los valores de prueba\n",
    "resultado = evalua_multiples_metodos(k_cv, k_knn, hiperparametro, x, y)\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1b401",
   "metadata": {},
   "source": [
    "Con los valores presentados, el modelo de análisis discriminante domina al de k vecinos cercanos y regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fde2c3",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "La elección del lambda mediante cross-validation se centra en elegir el valor de lambda que tiene un error cuadrático medio menor, en promedio, a otros valores de lambda. Tal como se presenta en la función evalua_config, el procedimiento para elegir este lambda óptimo consiste en partir la muestra original en dos partes: una de entrenamiento y otra de test. En la prueba de entrenamiento ajustamos nuestro modelo y luego en la de test observamos con qué precisión fueron predichos los valores que realmente se observan en esta muestra. A partir de esto se calcula el error cuadrático medio. El procedimiento de cross validation consiste en dividir nuestra muestra en k particiones y que cada partición pertenezca alguna vez a la muestra de test y k-1 veces a la muestra de entrenamiento. Para cada valor del lambda tendremos un conjunto de errores cuadráticos medios de acuerdo a las configuraciones de las particiones de la muestra. Una vez que obtenemos estos errores cuadráticos medios (serán un total de k) calculamos su promedio y comparamos con los que se obtuvieron con otros valores de lambda. Aquél que tenga el menor será el lambda óptimo.\n",
    "\n",
    "Al construir tu predicción con tu prueba de entrenamiento, evaluar el modelo en ella no tendría sentido, ya que el objetivo que se tiene en Big Data es saber la capacidad de predicción fuera de la muestra con la que se trabaja. Además, usar la base de prueba lleva a overfit y problema de sesgos en las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524c3f5",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "Es importante entender la lógica, si tomamos un k = 5 con 100 datos, cada muestra tiene 20 observaciones. Se dejan 20 afuera y se estima el modelo con las 80 restantes. Cada observación es utilizada una vez en la muestra de evaluación y cuatro veces en la muestra de entrenamiento.\n",
    "\n",
    "Cuando k = N se estima el modelo n veces con n - 1 datos. El problema que tenemos con un k demasiado chico es que entrenamos a nuestro modelo con una cantidad menor de datos, generando predicciones más imprecisas. Por el otro lado, cuando el k es demasiado grande nuestro modelo se entrena con una mayor cantidad de observaciones pero se testea en una menor cantidad, lo que puede generar problemas en el análisis de la verdadera precisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cd4af",
   "metadata": {},
   "source": [
    "### Ejercicio 5\n",
    "En primer lugar utilizamos nuestra función evalua_config para elegir el lambda óptimo de nuestra regresión logística para cada método de regularización. Además, imprimimos el error cuadrático medio para cada uno de los valores de lambda en cada método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eafc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparametro = 10**np.linspace(-5, 5, 11)\n",
    "resultado = evalua_config(hiperparametro, 10, x, y)\n",
    "hiperparametros_probados = resultado[3]  \n",
    "resultados_lasso = resultado[6]  # La lista de hiperparámetros probados\n",
    "resultado_ridge = resultado[5] \n",
    "print(\"El lambda que minimiza el error cuadrático medio en Lasso:\", resultado[1])\n",
    "print(\"El lambda que minimiza el error cuadrático medio en Ridge:\", resultado[2])\n",
    "print(\"resultados:\", resultado[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16702dce",
   "metadata": {},
   "source": [
    "En este paso generamos el boxplot para la especificación de Lasso. Para cada Lambda tendremos 10 observaciones del error cuadrático medio, una por cada partición de la muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados de Lasso en un boxplot\n",
    "plt.figure(figsize=(10, 8))\n",
    "resultados_lasso.boxplot(column='ecm', by='Lambda')\n",
    "plt.title(\"Resultados de Lasso\")\n",
    "plt.xlabel(\"Valor de Lambda\")\n",
    "plt.ylabel(\"Valor del ECM\")\n",
    "plt.xticks(rotation=30)  # Rotar las etiquetas del eje x\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c30c00",
   "metadata": {},
   "source": [
    "En este paso generamos el boxplot para la especificación de Lasso. Para cada Lambda tendremos 10 observaciones del error cuadrático medio, una por cada partición de la muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados de Ridge en un boxplot \n",
    "plt.figure(figsize=(10, 8))\n",
    "resultado_ridge.boxplot(column='ecm', by='Lambda')\n",
    "plt.title(\"Resultados de Ridge\")\n",
    "plt.xlabel(\"Valor de Lambda\")\n",
    "plt.ylabel(\"Valor del ECM\")\n",
    "plt.xticks(rotation=30)  # Rotar las etiquetas del eje x\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9f388",
   "metadata": {},
   "source": [
    "En este paso calculamos la proporción de coeficientes que, debido a la forma funcional de la penalidad en Lasso, se vuelven nulos. Esto, para cada uno de los lambda sobre los que iteramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "def calcular_coeficientes_lasso(hiperparametros, x, y):\n",
    "    '''\n",
    "    Esta función ajusta modelos de regresión Lasso con diferentes valores de alpha al conjunto de entrenamiento.\n",
    "    Inputs:\n",
    "        hiperparametros, x_train, y_train\n",
    "    Returns:\n",
    "        coeficientes finales\n",
    "    '''\n",
    "    coeficientes_finales = []\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "    \n",
    "    #Estandarizamos las variables para la regresión logística\n",
    "    sc = StandardScaler() \n",
    "    x_transformed_train = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "    x_transformed_test = pd.DataFrame(sc.fit_transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "   \n",
    "\n",
    "    for alpha in hiperparametros:\n",
    "        lassocv = LassoCV(alphas=[alpha], cv=10, max_iter=1000, random_state=100)\n",
    "        lassocv.fit(x_transformed_train, y_train) # Ajustamos el modelo\n",
    "        \n",
    "        coeficientes = pd.Series(lassocv.coef_, index=x_transformed_train.columns)\n",
    "        coeficientes_finales.append(coeficientes)\n",
    "\n",
    "    coeficientes_finales = pd.concat(coeficientes_finales, axis=1)\n",
    "\n",
    "    # Asignar nombres de columna basados en el valor de alpha\n",
    "    coeficientes_finales.columns = [f'{alpha}' for alpha in hiperparametros]\n",
    "    \n",
    "    return coeficientes_finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4deb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la función para calcular los coeficientes de Lasso para diferentes lambdas\n",
    "coeficientes_finales = calcular_coeficientes_lasso(hiperparametro, x, y)\n",
    "\n",
    "# Mostrar los coeficientes resultantes\n",
    "print(coeficientes_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este paso calculamos la proporción de coeficientes iguales a 0 para cada uno de los lambda\n",
    "coef_drop = pd.DataFrame()\n",
    "\n",
    "for column in coeficientes_finales.columns:\n",
    "    proportion_zeros = (coeficientes_finales[column] == 0).mean()\n",
    "    coef_drop[column] = [proportion_zeros]\n",
    "\n",
    "# Transponemos el DataFrame para que las columnas sean índices y viceversa\n",
    "coef_drop = coef_drop.T\n",
    "\n",
    "# Asignamos un nombre a la columna que contiene las proporciones de 0\n",
    "coef_drop.columns = ['Proporcion_Zeros']\n",
    "\n",
    "# Muestra el DataFrame coef_drop\n",
    "print(coef_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crea una figura con un tamaño más pequeño\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Crea el boxplot de coef_drop con los ejes intercambiados\n",
    "sns.boxplot(data=coef_drop, orient=\"v\")  # Cambiamos orient a \"v\" para vertical\n",
    "\n",
    "# Agrega etiquetas a los ejes\n",
    "plt.ylabel(\"Proporción de Valores 0\")\n",
    "plt.xlabel(\"Columnas\")\n",
    "\n",
    "# Rota la etiqueta en el eje X para que sea horizontal\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Muestra el plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debido a la escasa información revelada por el boxplot decidimos graficar también en un gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "coef_drop.plot(kind='bar', legend=False)\n",
    "plt.title('Proporción de Coeficientes Igual a 0 para Cada Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Proporción de Coeficientes Igual a 0')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71608b0",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "El valor óptimo encontrado por la función del ejercicio previo es 0.00001. En este primer paso llamamos a la columna correspondiente a este coeficiente de la base de datos creada en el ejercicio anterior. Debido a que la penalización es lo suficientemente baja, no encontramos ningún coeficiente igual a cero. Sin embargo, podemos observar que la variable IV12_3, referida a la cercanía de la vivienda a una villa de emergencia tiene un coeficiente que se acerca a 0. Al observar la base de datos de aquellos que respondieron el monto del ingreso familiar total, se puede notar que tan solo 9 personas informaron vivir cerca de un asentamiento y que solo tres de ellas era catalogada como pobre según los criterios del INDEC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos solo con la columna referida al lambda 0.00001\n",
    "primera_columna = coeficientes_finales.iloc[:, 0]  \n",
    "\n",
    "# Muestra la primera columna\n",
    "print(primera_columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53dfbf3",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "Elegir algun modelo de regresión logistica donde hayamos probado con distitnos parametros de regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos el proceso realizado en los ejercicios anteriores pero seteando el lambda de manera más amplia.\n",
    "hiperparametro = 10**np.linspace(-10, 10, 21)\n",
    "resultado = evalua_config(hiperparametro, 5, x, y)\n",
    "hiperparametros_probados = resultado[3]  # La lista de hiperparámetros probados\n",
    "print(\"El lambda de Lasso:\", resultado[1])\n",
    "print(\"El lambda de Ridge:\", resultado[2])\n",
    "print(\"resultados:\", resultado[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc97f7",
   "metadata": {},
   "source": [
    "Podemos observar que el mejor lambda para Lasso es igual a 0.1 donde el ECM es de 0.203151, por su parte el de Ridge es el 0.001 donde el ECM es de 0.203390. De esta forma, nos quedariamos con el modelo de Lasso que tiene un menor ECM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e21bea",
   "metadata": {},
   "source": [
    "### Ejercicio 8\n",
    "¿ Qué modelo predice mejor ? ¿ con qué hiperparametros?\n",
    "\n",
    "En primer lugar creamos una función capaz de hacer cross-validation para vecinos más cercanos, indicandole un rango de entre 2 y 10 vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48958fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el rango de veicnos más cercanos entre 2 y 10\n",
    "k_values = list(range(2, 11))\n",
    "ecm_results = []\n",
    "\n",
    "# Creamos un loop que itere a través de los valores que toma k \n",
    "for k in k_values:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "    ecm_scores = []\n",
    "  \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        x_test = x_test.values # Aseguramos que sea numpy, porque sino teniamos error\n",
    "        #Ajustamos el modelo\n",
    "        modelo_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        modelo_knn.fit(x_train, y_train)\n",
    "        y_pred = modelo_knn.predict(x_test)\n",
    "\n",
    "        # Calcular el ECM y agregarlo a la lista\n",
    "        ecm = mean_squared_error(y_test, y_pred)\n",
    "        ecm_scores.append(ecm)\n",
    "\n",
    "    # Calculamos el ecm promedio\n",
    "    ecm_mean = np.mean(ecm_scores)\n",
    "    ecm_results.append((k, ecm_mean))\n",
    "\n",
    "# Buscamos el vecino más cercano con el menor error cuadritco medio indicandole la columna dentro de la tupla\n",
    "best_k, min_ecm = min(ecm_results, key=lambda x: x[1])\n",
    "\n",
    "print(\"Resultados del ECM para diferentes valores de k:\")\n",
    "for k, ecm in ecm_results:\n",
    "    print(f\"k={k}: ECM promedio = {ecm}\")\n",
    "\n",
    "print(f\"El mejor valor de k con el menor ECM es k={best_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8cd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que el vecino más cercano con menor ecm es igual a 9: \n",
    "k_knn_values = 9\n",
    "# Utilizamos el rango de hiperparámetros deifnido\n",
    "hiperparam_values = 10**np.linspace(-5, 5, 11)\n",
    "# Partimos la base en 10 \n",
    "k_cv_values = 10\n",
    "\n",
    "# Creamos un valor de mejor ecm grande porque queremos el mínimo y una mejor configuración en vacio. \n",
    "best_ecm = float('inf')  \n",
    "best_config = None  \n",
    "\n",
    "for hiperparam in hiperparam_values:\n",
    "            resultado = evalua_multiples_metodos(k_cv_values, k_knn, [hiperparam],x, y)\n",
    "            ecm = resultado[resultado['Modelo'] == 'regresion_logistica']['ECM'].values[0]  \n",
    "            if ecm < best_ecm:\n",
    "                best_ecm = ecm\n",
    "                best_config = resultado\n",
    "\n",
    "print(\"Mejor configuración:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4728e",
   "metadata": {},
   "source": [
    "Observamos, luego de utilizar la función evalua_multiples_metodos que el mejor modelo para predecir con los parámetros evaluados es el de Ánalisis discrimiante que posee una precisión superior a los otros dos, un valor AUC levemente inferior al de regresión logistica, pero un error cuadrático medio menor a los otros dos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cfbcb3",
   "metadata": {},
   "source": [
    "### Ejercicio 9\n",
    "Con el método que seleccionamos, predecimos los pobres dentro de la base no respondieron. indicar la proproción de los hoagres pobres en esa submuestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos la base de datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=201)\n",
    "# Calculamos el número de componentes a utilizar en el análisis de discriminante lineal (LDA)\n",
    "# Nota: limitaciones de LDA, que no puede tener más componentes que el número de clases menos 1.\n",
    "n_components = min(X_train.shape[1], len(set(Y_train)) - 1)\n",
    "lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "lda = lda.fit(X_train, Y_train)\n",
    "X_r = lda.transform(X_train)\n",
    "Y_test_pred_lda = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la variable Y según el método entrenado\n",
    "Xp = no_respondieron\n",
    "Xp = pd.concat([pd.Series(1, index=Xp.index, name='constante'), Xp], axis=1)\n",
    "\n",
    "Y_norespondieron_pred = lda.predict(Xp)\n",
    "\n",
    "# Calcula la proporción de personas \"pobres\" y \"no pobres\"\n",
    "proporcion_pobres = (Y_norespondieron_pred == 1).mean()\n",
    "proporcion_no_pobres = (Y_norespondieron_pred == 0).mean()\n",
    "\n",
    "print(\"Proporción de personas consideradas 'pobres':\", proporcion_pobres)\n",
    "print(\"Proporción de personas consideradas 'no pobres':\", proporcion_no_pobres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4225b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregar columna de pobre en base a la predicción\n",
    "no_respondieron['Pobre'] = Y_norespondieron_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f864530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dejamos la base de datos de copia igual que la de no respondieron y respondieron orginal\n",
    "# a excepción de las variables CODUSU y PONDIH\n",
    "columnas_a_eliminar = ['ITF','IPCF', 'P47T','adulto_equiv','ad_equiv_hogar']\n",
    "respondieron_copia = respondieron_copia.drop (columns=columnas_a_eliminar)\n",
    "respondieron_copia = respondieron_copia.drop(columns=['ingreso_necesario'])\n",
    "no_respondieron_copia = no_respondieron_copia.drop (columns=columnas_a_eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos a no_respondieron_copia la variable Pobre generada en no_respondieron\n",
    "no_respondieron_copia['Pobre'] = no_respondieron['Pobre']\n",
    "# Agrupamos los datos por el código del hogar (CODUSU)\n",
    "no_respondieron_copia = no_respondieron_copia.groupby('CODUSU').first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la proporción de hogares \"pobres\" y \"no pobres\"\n",
    "proporcion_pobres_hogares = (no_respondieron_copia['Pobre'] == 1).mean()\n",
    "proporcion_no_pobres_hogares = (no_respondieron_copia['Pobre'] == 0).mean()\n",
    "\n",
    "print(\"Proporción de hogares consideradas 'pobres':\", proporcion_pobres_hogares)\n",
    "print(\"Proporción de hogares consideradas 'no pobres':\", proporcion_no_pobres_hogares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23fe8e",
   "metadata": {},
   "source": [
    "Observamos que la proporción de hogares pobres estimada en la base donde no respondieron los ingresos es menor a la estimada para toda la pobalción en base a la data donde respondieron los ingresos"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
